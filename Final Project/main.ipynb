{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "#\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "pd.options.display.float_format = '{:.3f}'.format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"orange\">  **Data Loading and Visualizaiton** </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('health_insurance_train.csv')\n",
    "df_autograder = pd.read_csv('health_insurance_autograde.csv')\n",
    "                            \n",
    "display(df.iloc[8:16])\n",
    "display(df_autograder.iloc[8:16])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"orange\">  **Data processing and data pipelines** </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_1(df):\n",
    "    # turns all data numeric except regions\n",
    "    education_mapping = {\n",
    "        '<9years': 8,      # Assume '<9years' corresponds to 8 years\n",
    "        '9-11years': 10,   # Midpoint for '9-11years'\n",
    "        '12years': 12,     # Exact number of years\n",
    "        '11-13years': 12,  # Midpoint for '11-13years'\n",
    "        '13-15years': 14,  # Midpoint for '13-15years'\n",
    "        '16years': 16,     # Exact number of years\n",
    "        '>16years': 18     # Assume '>16years' corresponds to 17 years\n",
    "    }\n",
    "    \n",
    "    yn_mapping = {'yes': 1, 'no': 0}\n",
    "    race_mapping = {'white': 1, 'black': 0}\n",
    "    \n",
    "    df['education'] = df['education'].map(education_mapping)\n",
    "    df['race'] = df['race'].map(race_mapping).fillna(0.5)\n",
    "    \n",
    "    binary_columns = ['hhi', 'whi', 'hhi2', 'hispanic']\n",
    "    for col in binary_columns:\n",
    "        df[col] = df[col].map(yn_mapping)\n",
    "    \n",
    "    df['kidslt6'] = df['kidslt6'].fillna(df['kidslt6'].median())\n",
    "    df['kids618'] = df['kids618'].fillna(df['kids618'].median())\n",
    "    \n",
    "    \n",
    "    display(df.iloc[8:16])\n",
    "    return df\n",
    "\n",
    "def onehotencode(df):\n",
    "    df = pd.get_dummies(df, columns=['region'],prefix='reg', drop_first=True)\n",
    "    tf_mapping = {True: 1, False: 0}\n",
    "    cols = ['reg_other', 'reg_south', 'reg_west', 'reg_northcentral']\n",
    "    for col in cols:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].map(tf_mapping)\n",
    "    return df\n",
    "\n",
    "def visualize_data(df):\n",
    "    # 1. Histograms\n",
    "    df.hist(figsize=(12, 8), bins=10, color='skyblue', edgecolor='black')\n",
    "    plt.suptitle('Histograms of dfset Features')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # 2. Pairplot - Showing pairwise relationships\n",
    "    # sns.pairplot(df)\n",
    "    # plt.suptitle('Pairwise Plot of Features')\n",
    "    # plt.show()\n",
    "    \n",
    "    # 3. Correlation heatmap\n",
    "    if 'region' in df.columns:\n",
    "        data = df.drop('region', axis=1)\n",
    "    else:\n",
    "        data = df.copy()\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.heatmap(data.corr(), annot=True, cmap='coolwarm', linewidths=0.5)\n",
    "    plt.title('Correlation Heatmap of Features')\n",
    "    plt.show()\n",
    "\n",
    "df = preprocess_1(df)\n",
    "df = onehotencode(df)\n",
    "\n",
    "df_autograder = onehotencode(df_autograder)\n",
    "df_autograder = preprocess_1(df_autograder)\n",
    "\n",
    "visualize_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaling_all(df):\n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(df)\n",
    "    return pd.DataFrame(X, columns=df.columns, index=df.index)\n",
    "\n",
    "def scaling_selective(df, cols):\n",
    "    for col in cols:\n",
    "        df[col] = (df[col] -df[col].mean()) / df[col].std()\n",
    "    return df\n",
    "\n",
    "def remove_mahalanobis_outliers(df, percentile=98):\n",
    "    \"\"\"\n",
    "    Remove outliers based on Mahalanobis distance from a DataFrame's numerical columns.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame.\n",
    "        percentile (float): The percentile to use as a threshold for identifying outliers (default is 98).\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame with outliers removed.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Step 1: Select only numerical columns\n",
    "    df_numeric = df.select_dtypes(include=[np.number])\n",
    "    \n",
    "    # Step 2: Calculate the mean vector and covariance matrix\n",
    "    mean_vector = df_numeric.mean(axis=0)\n",
    "    cov_matrix = np.cov(df_numeric.values.T)\n",
    "\n",
    "    # Step 3: Add a small regularization term to the covariance matrix\n",
    "    regularization_term = 1e-5 * np.eye(cov_matrix.shape[0])\n",
    "    cov_matrix += regularization_term\n",
    "    \n",
    "    # Step 4: Mahalanobis distance function\n",
    "    def mahalanobis_distance(row, mean_vector, cov_matrix):\n",
    "        diff = row - mean_vector\n",
    "        inv_cov_matrix = np.linalg.inv(cov_matrix)\n",
    "        md = np.sqrt(diff.T @ inv_cov_matrix @ diff)\n",
    "        return md\n",
    "    \n",
    "    # Step 5: Apply the Mahalanobis distance function to each row\n",
    "    df_filtered = df.copy()\n",
    "    df_filtered['mahalanobis'] = df_numeric.apply(lambda row: mahalanobis_distance(row, mean_vector, cov_matrix), axis=1)\n",
    "    \n",
    "    # Step 6: Determine the threshold for identifying outliers\n",
    "    threshold = np.percentile(df_filtered['mahalanobis'], percentile)\n",
    "    \n",
    "    # Step 7: Plot the distribution of Mahalanobis distances\n",
    "    plt.hist(df_filtered['mahalanobis'], bins=30, edgecolor='k', alpha=0.7, density=True, label='Mahalanobis Distance')\n",
    "    plt.axvline(threshold, color='r', linestyle='dashed', linewidth=1, label=f'Threshold (at {percentile}th percentile)')\n",
    "    plt.title('Distribution of Mahalanobis Distances')\n",
    "    plt.xlabel('Mahalanobis Distance')\n",
    "    plt.ylabel('Density')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    # Step 8: Identify and filter out the outliers\n",
    "    outliers = df_filtered[df_filtered['mahalanobis'] > threshold]\n",
    "    display(outliers.iloc[0:20])\n",
    "    \n",
    "    if not outliers.empty:\n",
    "        print(\"Outliers found\")\n",
    "        print(f\"Number of outliers: {len(outliers)}\")\n",
    "    else:\n",
    "        print(\"No outliers found\")\n",
    "    \n",
    "    # Step 9: Remove outliers and drop the Mahalanobis column\n",
    "    df_filtered = df_filtered[df_filtered['mahalanobis'] <= threshold].drop(columns=['mahalanobis'])\n",
    "    \n",
    "    return df_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_mean = df['whrswk'].mean()\n",
    "Y_std = df['whrswk'].std()\n",
    "print(f'Mean of whrswk: {Y_mean}')\n",
    "print(f'Standard deviation of whrswk: {Y_std}')\n",
    "\n",
    "###############################################################\n",
    "#------------------------ PIPE 1 ------------------------------#\n",
    "\n",
    "df_1 = scaling_all(df)\n",
    "df_autograder_1 = scaling_all(df_autograder)\n",
    "\n",
    "display(df_1.iloc[8:16])\n",
    "display(df_autograder_1.iloc[8:16])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################\n",
    "#------------------------ PIPE 2 ------------------------------#\n",
    "\n",
    "df_2 = scaling_selective(df , ['whrswk','experience', 'kidslt6', 'kids618', 'husby', 'education'])\n",
    "df_2 = remove_mahalanobis_outliers(df_2, percentile=98)\n",
    "\n",
    "df_autograder_2 = scaling_selective(df_autograder,['experience', 'kidslt6', 'kids618', 'husby', 'education'])\n",
    "\n",
    "display(df_2.iloc[8:16])\n",
    "display(df_autograder_2.iloc[8:16])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into seen and unseen while keeping it as a pandas dataframe\n",
    "fraction = 0.2  # 20% of the rows\n",
    "\n",
    "#-------------------PIPE 1-------------------\n",
    "df_unseen_1 = df_1.sample(frac = fraction, random_state=42) # Get 20% of random rows\n",
    "df_seen_1 = df_1.drop(df_unseen_1.index) # Get the remaining 80% of the rows\n",
    "\n",
    "X_seen_1 = df_seen_1.iloc[:, 1:]\n",
    "Y_seen_1 = df_seen_1.iloc[:, 0]\n",
    "\n",
    "X_unseen_1 = df_unseen_1.iloc[:, 1:]\n",
    "Y_unseen_1 = df_unseen_1.iloc[:, 0]\n",
    "\n",
    "#-------------------PIPE 2-------------------\n",
    "df_unseen_2 = df_2.sample(frac = fraction, random_state=42)\n",
    "df_seen_2 = df_2.drop(df_unseen_2.index)\n",
    "\n",
    "\n",
    "\n",
    "X_seen_2 = df_seen_2.iloc[:, 1:]\n",
    "Y_seen_2 = df_seen_2.iloc[:, 0]\n",
    "\n",
    "X_unseen_2 = df_unseen_2.iloc[:, 1:]\n",
    "Y_unseen_2 = df_unseen_2.iloc[:, 0]\n",
    "\n",
    "print('------------------------PIPE 1------------------------')\n",
    "print(f'Shape unseen data: {df_unseen_1.shape}')\n",
    "print(f'Shape seen data: {df_seen_1.shape}')\n",
    "\n",
    "print('------------------------PIPE 2------------------------')\n",
    "print(f'Shape unseen data: {df_unseen_2.shape}')\n",
    "print(f'Shape seen data: {df_seen_2.shape}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"orange\">  **Training Functions** </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#-------------------Dummy-------------------\n",
    "from sklearn.dummy import DummyRegressor\n",
    "def train_dummy_predictor(X, Y):\n",
    "    model = DummyRegressor(strategy='mean')\n",
    "    model.fit(X, Y)\n",
    "    return model\n",
    "\n",
    "#-------------------KNN-------------------\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "def train_knn_regressor(X, Y, param_grid):\n",
    "    model = KNeighborsRegressor(**param_grid)\n",
    "    model.fit(X, Y)\n",
    "    Y_pred = model.predict(X)\n",
    "    loss_values = mean_absolute_error(Y, Y_pred)\n",
    "    return model,loss_values\n",
    "\n",
    "#-------------------SGD-------------------\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "def train_sgd_regressor(X, Y, params):\n",
    "    model = SGDRegressor(**params)\n",
    "    epochs = params['max_iter']\n",
    "\n",
    "    #For SGD we want store loss during training for visualization\n",
    "    loss_values = []\n",
    "    for epoch in range(epochs):\n",
    "        model.partial_fit(X, Y)\n",
    "        Y_pred = model.predict(X)\n",
    "        epoch_loss = mean_absolute_error(Y, Y_pred)\n",
    "        loss_values.append(epoch_loss)\n",
    "    \n",
    "    return model, loss_values\n",
    "\n",
    "#-----------Decision Tree-------------------\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "def train_decision_tree_regressor(X, Y, params):\n",
    "\n",
    "    model = DecisionTreeRegressor(**params,random_state = 42)\n",
    "    loss_values = []\n",
    "    \n",
    "    #For Decision Tree we also want store loss during training for visualization\n",
    "    for depth in range(1, params['max_depth'] + 1):\n",
    "        model.set_params(max_depth=depth)\n",
    "        model.fit(X, Y)\n",
    "        Y_pred = model.predict(X)\n",
    "        loss = mean_absolute_error(Y, Y_pred)\n",
    "        loss_values.append(loss)\n",
    "    \n",
    "    return model, loss_values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"orange\">  **Logistic Adjustment** </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "def soft_max_adjustnent(value, target1 = -1.372319745743416, target2= 0.765866095571318 , gamma=5):\n",
    "    # Calculate the distance to each target\n",
    "    dist_to_target1 = abs(value - target1)\n",
    "    dist_to_target2 = abs(value - target2)\n",
    "    \n",
    "    # Transform the distance to probability\n",
    "    # Assumed the distance follows a Exponential distribution\n",
    "    prob_target1 = np.exp(-gamma * dist_to_target1)\n",
    "    prob_target2 = np.exp(-gamma * dist_to_target2)\n",
    "    \n",
    "    # Normalize the probabilities using soft max\n",
    "    total_prob = prob_target1 + prob_target2\n",
    "    prob_target1 /= total_prob\n",
    "    prob_target2 /= total_prob\n",
    "    \n",
    "    # Adjust the value based on the probabilities\n",
    "    adjusted_value = prob_target1 * target1 + prob_target2 * target2\n",
    "    \n",
    "    return adjusted_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"orange\">  **Create model dict and test dataframe** </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#----------Creation of models dict-----------\n",
    "models_dict_1 = {'KNN': None, 'SGD': None, 'Tree':None, 'RF':None}\n",
    "models_dict_2 = {'KNN': None, 'SGD': None, 'Tree':None, 'RF':None}\n",
    "\n",
    "for key in models_dict_1:\n",
    "    models_dict_1[key] = {'default' :None, 'best_param':None, 'best_model' :None, 'ensemble':None, 'best_ensemble':None}\n",
    "    models_dict_2[key] = {'default' :None, 'best_param':None, 'best_model' :None, 'ensemble':None, 'best_ensemble':None}\n",
    "\n",
    "#----------Creation of test dfs-----------\n",
    "test_df_1 = pd.DataFrame(index=['D','T','TA','ET','BET','BETA'],columns=['KNN','SGD','Tree','RF'])\n",
    "test_df_2 = pd.DataFrame(index=['D','T','TA','ET','BET','BETA'],columns=['KNN','SGD','Tree','RF'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"orange\">  **Store and test default models** </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################\n",
    "# ------ KNN REGRESSOR\n",
    "model = KNeighborsRegressor()\n",
    "model.fit(X_seen_1, Y_seen_1)\n",
    "models_dict_1['KNN']['default'] = model\n",
    "test_df_1.loc['D','KNN'] = mean_absolute_error(Y_unseen_1, model.predict(X_unseen_1))\n",
    "\n",
    "model = KNeighborsRegressor()\n",
    "model.fit(X_seen_2, Y_seen_2)\n",
    "models_dict_2['KNN']['default'] = model\n",
    "test_df_2.loc['D','KNN'] = mean_absolute_error(Y_unseen_2, model.predict(X_unseen_2))\n",
    "\n",
    "\n",
    "####################################################################\n",
    "# ------ SGD REGRESSOR\n",
    "model = SGDRegressor()\n",
    "model.fit(X_seen_1, Y_seen_1)\n",
    "models_dict_1['SGD']['default'] = model\n",
    "test_df_1.loc['D','SGD'] = mean_absolute_error(Y_unseen_1, model.predict(X_unseen_1))\n",
    "\n",
    "model = SGDRegressor()\n",
    "model.fit(X_seen_2,  Y_seen_2)\n",
    "models_dict_2['SGD']['default'] = model\n",
    "test_df_2.loc['D','SGD'] = mean_absolute_error(Y_unseen_2, model.predict(X_unseen_2))\n",
    "\n",
    "####################################################################\n",
    "# ------ DECISION TREE REGRESSOR\n",
    "model = DecisionTreeRegressor()\n",
    "model.fit(X_seen_1, Y_seen_1)\n",
    "models_dict_1['Tree']['default'] = model\n",
    "test_df_1.loc['D','Tree'] = mean_absolute_error(Y_unseen_1, model.predict(X_unseen_1))\n",
    "\n",
    "model = DecisionTreeRegressor()\n",
    "model.fit(X_seen_2, Y_seen_2)\n",
    "models_dict_2['Tree']['default'] = model\n",
    "test_df_2.loc['D','Tree'] = mean_absolute_error(Y_unseen_2, model.predict(X_unseen_2))\n",
    "\n",
    "####################################################################\n",
    "# ------ RANDOM FOREST REGRESSOR\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "model = RandomForestRegressor()\n",
    "model.fit(X_seen_1, Y_seen_1)\n",
    "models_dict_1['RF']['default'] = model\n",
    "test_df_1.loc['D','RF'] = mean_absolute_error(Y_unseen_1, model.predict(X_unseen_1))\n",
    "\n",
    "model = RandomForestRegressor()\n",
    "model.fit(X_seen_2, Y_seen_2)\n",
    "models_dict_2['RF']['default'] = model\n",
    "test_df_2.loc['D','RF'] = mean_absolute_error(Y_unseen_2, model.predict(X_unseen_2))\n",
    "\n",
    "display(pd.concat([test_df_1,test_df_2],axis=1)*Y_std)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"orange\">  **Perform grid search, test, store best models and parameters** </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def grid_search(X, Y, model, param_grid, cv=5):\n",
    "    \n",
    "    # cv = 5,  determines the cross-validation splitting strategy\n",
    "    # cv = 5 is the standard but after some testing it seems to be the best option\n",
    "\n",
    "    grid_search = GridSearchCV(estimator=model, param_grid=param_grid, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1)\n",
    "    \n",
    "    # Fit the model\n",
    "    print(\"Working on grid search\")\n",
    "    grid_search.fit(X, Y)\n",
    "    \n",
    "    # Get the best model and parameters\n",
    "    best_model = grid_search.best_estimator_\n",
    "    best_params = grid_search.best_params_\n",
    "    \n",
    "    print(f\"Best parameters: {best_params}\\n\")\n",
    "    \n",
    "    return best_model, best_params\n",
    "\n",
    "\n",
    "def search_train_test_store(model, param_grid, X_seen, Y_seen, X_unseen, Y_unseen, models_dict, model_name, test_df):\n",
    "    '''\n",
    "    This function will apply the grid search given the grid and model type. \n",
    "    Save the best parameters and model, perform a test on the unseen data\n",
    "    and store the results in the test_df.\n",
    "    '''\n",
    "\n",
    "\n",
    "    # Perform grid search\n",
    "    best_model, best_params = grid_search(X_seen, Y_seen, model, param_grid, cv=5)\n",
    "    models_dict[model_name]['best_model'] = best_model\n",
    "    models_dict[model_name]['best_param'] = best_params\n",
    "    \n",
    "    # Predict and calculate errors\n",
    "    Y_pred = best_model.predict(X_unseen)\n",
    "    test_df.loc['T', model_name] = mean_absolute_error(Y_unseen, Y_pred)\n",
    "    Y_pred_adjusted = np.array([soft_max_adjustnent(value) for value in Y_pred])\n",
    "    test_df.loc['TA', model_name] = mean_absolute_error(Y_unseen, Y_pred_adjusted)\n",
    "\n",
    "# Define parameter grids\n",
    "param_grid_KNN = {\n",
    "    'n_neighbors': [3, 5, 7, 9, 11, 13, 15, 17, 19, 21],\n",
    "    'weights': ['uniform','distance'],\n",
    "    'p': [1, 2]  # 1 for Manhattan distance, 2 for Euclidean distance\n",
    "}\n",
    "\n",
    "param_grid_SGD = {\n",
    "    'alpha': [0.00001,0.0001],\n",
    "    'penalty': ['l2', 'l1', 'elasticnet'],\n",
    "    'learning_rate': ['constant', 'optimal', 'invscaling', 'adaptive'],\n",
    "    'eta0': [0.00001, 0.0001],\n",
    "    'max_iter': [25, 50, 100],\n",
    "}\n",
    "\n",
    "param_grid_Tree = {\n",
    "    'criterion': ['squared_error', 'absolute_error'],\n",
    "    'splitter': ['random'], # Random worked better than best almost every time\n",
    "    'max_depth': [8 , 10, 15, 20],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4, 8],\n",
    "    'max_features': [None] # All features are considered for each split\n",
    "}\n",
    "\n",
    "param_grid_RF = {\n",
    "    'n_estimators': [50, 100],\n",
    "    'max_depth': [8, 10, 15],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4, 8],\n",
    "    'max_features': [None, 'sqrt', 'log2']\n",
    "}\n",
    "\n",
    "print(\"------ KNN Regressor -----\")\n",
    "search_train_test_store(KNeighborsRegressor(), param_grid_KNN, X_seen_1, Y_seen_1, X_unseen_1, Y_unseen_1, models_dict_1, 'KNN', test_df_1)\n",
    "search_train_test_store(KNeighborsRegressor(), param_grid_KNN, X_seen_2, Y_seen_2, X_unseen_2, Y_unseen_2, models_dict_2, 'KNN', test_df_2)\n",
    "\n",
    "print(\"------ SGD Regressor -----\")\n",
    "search_train_test_store(SGDRegressor(random_state=42), param_grid_SGD, X_seen_1, Y_seen_1, X_unseen_1, Y_unseen_1, models_dict_1, 'SGD', test_df_1)\n",
    "search_train_test_store(SGDRegressor(random_state=42), param_grid_SGD, X_seen_2, Y_seen_2, X_unseen_2, Y_unseen_2, models_dict_2, 'SGD', test_df_2)\n",
    "\n",
    "print(\"------ Decision Tree Regressor -----\")\n",
    "search_train_test_store(DecisionTreeRegressor(random_state=42), param_grid_Tree, X_seen_1, Y_seen_1, X_unseen_1, Y_unseen_1, models_dict_1, 'Tree', test_df_1)\n",
    "search_train_test_store(DecisionTreeRegressor(random_state=42), param_grid_Tree, X_seen_2, Y_seen_2, X_unseen_2, Y_unseen_2, models_dict_2, 'Tree', test_df_2)\n",
    "\n",
    "print(\"------ Random Forest Regressor -----\")\n",
    "search_train_test_store(RandomForestRegressor(random_state=42), param_grid_RF, X_seen_1, Y_seen_1, X_unseen_1, Y_unseen_1, models_dict_1, 'RF', test_df_1)\n",
    "search_train_test_store(RandomForestRegressor(random_state=42), param_grid_RF, X_seen_2, Y_seen_2, X_unseen_2, Y_unseen_2, models_dict_2, 'RF', test_df_2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display results\n",
    "display(pd.concat([test_df_1,test_df_2],axis=1) * Y_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"orange\">  **Ensemble training and validation loss** </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_store_ensemble(models_dict, X_seen, Y_seen, regressor_type, train_function, n_models=10, val_size=0.2):\n",
    "    '''\n",
    "    This function will train multiple models with the parameters given, based on different data splits.\n",
    "    It will store the trained models in the models_dict, and return a list of training losses and validation losses.\n",
    "    '''\n",
    "\n",
    "    models_training_loss = []\n",
    "    models_val_loss = []\n",
    "    model_list = []\n",
    "\n",
    "    params = models_dict[regressor_type]['best_param']\n",
    "\n",
    "    for n in range(n_models):\n",
    "        X_train, X_val, Y_train, Y_val = train_test_split(X_seen, Y_seen, test_size=val_size, random_state=42*n)\n",
    "\n",
    "        model, loss_values = train_function(X_train, Y_train, params)\n",
    "        \n",
    "        model_list.append(model)\n",
    "        models_training_loss.append(loss_values)\n",
    "\n",
    "        Y_pred = model.predict(X_val)\n",
    "        val_loss = mean_absolute_error(Y_val, Y_pred)\n",
    "        models_val_loss.append(val_loss)\n",
    "\n",
    "    models_dict[regressor_type]['ensemble'] = model_list\n",
    "\n",
    "    return models_training_loss, models_val_loss\n",
    "\n",
    "n_models = 25\n",
    "\n",
    "# Create a dataframe to store the validation loss of each model with the mean at the end\n",
    "Ensembles_val_loss_1 = pd.DataFrame(index=range(n_models + 1))\n",
    "Ensembles_val_loss_1.rename(index={n_models: 'mean'}, inplace=True)\n",
    "\n",
    "Ensembles_val_loss_2 = pd.DataFrame(index=range(n_models + 1))\n",
    "Ensembles_val_loss_2.rename(index={n_models: 'mean'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#-------------------PIPE 1-------------------\n",
    "print(\"------ Training ensembles for PIPE 1 ------\")\n",
    "\n",
    "models_training_loss_SGD_1, Ensembles_val_loss_1.loc[:n_models - 1, 'SGD'] = train_and_store_ensemble(models_dict_1, X_seen_1, Y_seen_1, 'SGD', train_sgd_regressor, n_models)\n",
    "Ensembles_val_loss_1.loc['mean', 'SGD'] = Ensembles_val_loss_1['SGD'].mean()\n",
    "print(\" SGD : Done\")\n",
    "\n",
    "models_training_loss_KNN_1, Ensembles_val_loss_1.loc[:n_models - 1, 'KNN'] = train_and_store_ensemble(models_dict_1, X_seen_1, Y_seen_1, 'KNN', train_knn_regressor, n_models)\n",
    "Ensembles_val_loss_1.loc['mean', 'KNN'] = Ensembles_val_loss_1['KNN'].mean()\n",
    "print(\" KNN : Done\")\n",
    "\n",
    "models_training_loss_Tree_1, Ensembles_val_loss_1.loc[:n_models - 1, 'Tree'] = train_and_store_ensemble(models_dict_1, X_seen_1, Y_seen_1, 'Tree', train_decision_tree_regressor, n_models)\n",
    "Ensembles_val_loss_1.loc['mean', 'Tree'] = Ensembles_val_loss_1['Tree'].mean()\n",
    "print(\" Tree : Done\")\n",
    "\n",
    "#-------------------PIPE 2-------------------\n",
    "print(\"------ Training ensembles for PIPE 2 ------\")\n",
    "\n",
    "models_training_loss_SGD_2, Ensembles_val_loss_2.loc[:n_models - 1, 'SGD'] = train_and_store_ensemble(models_dict_2, X_seen_2, Y_seen_2, 'SGD', train_sgd_regressor, n_models)\n",
    "Ensembles_val_loss_2.loc['mean', 'SGD'] = Ensembles_val_loss_2['SGD'].mean()\n",
    "print(\" SGD : Done\")\n",
    "\n",
    "models_training_loss_KNN_2, Ensembles_val_loss_2.loc[:n_models - 1, 'KNN'] = train_and_store_ensemble(models_dict_2, X_seen_2, Y_seen_2, 'KNN', train_knn_regressor, n_models)\n",
    "Ensembles_val_loss_2.loc['mean', 'KNN'] = Ensembles_val_loss_2['KNN'].mean()\n",
    "print(\" KNN : Done\")\n",
    "\n",
    "models_training_loss_Tree_2, Ensembles_val_loss_2.loc[:n_models - 1, 'Tree'] = train_and_store_ensemble(models_dict_2, X_seen_2, Y_seen_2, 'Tree', train_decision_tree_regressor, n_models)\n",
    "Ensembles_val_loss_2.loc['mean', 'Tree'] = Ensembles_val_loss_2['Tree'].mean()\n",
    "print(\" Tree : Done\")\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(models_training_loss_KNN_2)\n",
    "\n",
    "fig, ax = plt.subplots(2, 3, figsize=(20, 5))\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(hspace=0.5)\n",
    "\n",
    "for loss_values in models_training_loss_SGD_1[::3]: # Plot every 3rd model\n",
    "    ax[0, 0].plot(loss_values)\n",
    "\n",
    "ax[0, 0].set_ylabel('Mean Absolute Error')\n",
    "ax[0, 0].set_title('SGD Ensemble Training Loss')\n",
    "ax[0, 0].set_xlabel('Epoch')\n",
    "ax[0, 0].set_ylim(0, 1)\n",
    "\n",
    "for loss_values in models_training_loss_SGD_2[::3]: # Plot every 3rd model\n",
    "    ax[1, 0].plot(loss_values)\n",
    "\n",
    "ax[1, 0].set_ylabel('Mean Absolute Error')\n",
    "ax[1, 0].set_title('SGD Ensemble Training Loss')\n",
    "ax[1, 0].set_xlabel('Epoch')\n",
    "ax[1, 0].set_ylim(0, 1)\n",
    "\n",
    "ax[0, 1].hist([value for value in models_training_loss_KNN_1], bins=30, edgecolor='k', alpha=0.7)\n",
    "ax[0, 1].set_title('KNN Ensemble Training Loss')\n",
    "ax[0, 1].set_ylabel('Frequency')\n",
    "\n",
    "\n",
    "ax[1, 1].hist([value for value in models_training_loss_KNN_2], bins=30, edgecolor='k', alpha=0.7)\n",
    "ax[1, 1].set_title('KNN Ensemble Training Loss')\n",
    "ax[1, 1].set_ylabel('Frequency')\n",
    "\n",
    "for loss_values in models_training_loss_Tree_1[::3]: # Plot every 3rd model\n",
    "    ax[0, 2].plot(loss_values)\n",
    "\n",
    "ax[0, 2].set_ylabel('Mean Absolute Error')\n",
    "ax[0, 2].set_title('Decision Tree Ensemble Training Loss')\n",
    "ax[0, 2].set_xlabel('Depth')\n",
    "\n",
    "for loss_values in models_training_loss_Tree_2[::3]: # Plot every 3rd model\n",
    "    ax[1, 2].plot(loss_values)\n",
    "\n",
    "ax[1, 2].set_ylabel('Mean Absolute Error')\n",
    "ax[1, 2].set_title('Decision Tree Ensemble Training Loss')\n",
    "ax[1, 2].set_xlabel('Depth')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "display(pd.concat([Ensembles_val_loss_1, Ensembles_val_loss_2],axis=1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"orange\">  **Ensemble tests** </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble_predict(models_dict, model_type, X_unseen, Y_unseen, test_df):\n",
    "    ensemble_pred = [model.predict(X_unseen) for model in models_dict[model_type]['ensemble']]\n",
    "    test_pred = np.mean(ensemble_pred, axis=0)\n",
    "    test_df.loc['ET', model_type] = mean_absolute_error(Y_unseen, test_pred)\n",
    "\n",
    "    return test_pred\n",
    "\n",
    "# KNN predictions\n",
    "Y_unseen_pred_KNN_1 = ensemble_predict(models_dict_1, 'KNN', X_unseen_1, Y_unseen_1, test_df_1)\n",
    "Y_unseen_pred_KNN_2 = ensemble_predict(models_dict_2, 'KNN', X_unseen_2, Y_unseen_2, test_df_2)\n",
    "\n",
    "# SGD predictions\n",
    "Y_unseen_pred_SGD_1 = ensemble_predict(models_dict_1, 'SGD', X_unseen_1, Y_unseen_1, test_df_1)\n",
    "Y_unseen_pred_SGD_2 = ensemble_predict(models_dict_2, 'SGD', X_unseen_2, Y_unseen_2, test_df_2)\n",
    "\n",
    "# Tree predictions\n",
    "Y_unseen_pred_Tree_1 = ensemble_predict(models_dict_1, 'Tree', X_unseen_1, Y_unseen_1, test_df_1)\n",
    "Y_unseen_pred_Tree_2 = ensemble_predict(models_dict_2, 'Tree', X_unseen_2, Y_unseen_2, test_df_2)\n",
    "\n",
    "def plot_histograms(Y_unseen, Y_unseen_pred_KNN, Y_unseen_pred_SGD, Y_unseen_pred_Tree):\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "    counts, bin_edges = np.histogram(Y_unseen, bins=30)\n",
    "\n",
    "    axs[0, 0].hist(Y_unseen, bins=bin_edges, edgecolor='k', alpha=0.7, density=True, label='Test Data')\n",
    "    axs[0, 0].legend()\n",
    "\n",
    "    axs[0, 1].hist(Y_unseen, bins=bin_edges, edgecolor='k', alpha=0.7, density=True, label='Test Data')\n",
    "    axs[0, 1].hist(Y_unseen_pred_KNN, bins=bin_edges, edgecolor='k', alpha=0.7, density=True, label='KNN Ensemble')\n",
    "    axs[0, 1].legend()\n",
    "\n",
    "    axs[1, 0].hist(Y_unseen, bins=bin_edges, edgecolor='k', alpha=0.7, density=True, label='Test Data')\n",
    "    axs[1, 0].hist(Y_unseen_pred_SGD, bins=bin_edges, edgecolor='k', alpha=0.7, density=True, label='SGD Ensemble')\n",
    "    axs[1, 0].legend()\n",
    "\n",
    "    axs[1, 1].hist(Y_unseen, bins=bin_edges, edgecolor='k', alpha=0.7, density=True, label='Test Data')\n",
    "    axs[1, 1].hist(Y_unseen_pred_Tree, bins=bin_edges, edgecolor='k', alpha=0.7, density=True, label='Random Forest')\n",
    "    axs[1, 1].legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot histograms for the first dataset\n",
    "plot_histograms(Y_unseen_1, Y_unseen_pred_KNN_1, Y_unseen_pred_SGD_1, Y_unseen_pred_Tree_1)\n",
    "\n",
    "# Plot histograms for the second dataset\n",
    "plot_histograms(Y_unseen_2, Y_unseen_pred_KNN_2, Y_unseen_pred_SGD_2, Y_unseen_pred_Tree_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_best_models(models_dict, Ensembles_val_loss, model_type, selected_models):\n",
    "    '''\n",
    "    This function sorts the models in ascending order based on their validation loss.\n",
    "    Then it takes their indexes and uses it to retrieve the best models from the models_dict[model_type]['ensemble'].\n",
    "    '''\n",
    "\n",
    "    selected_indices = Ensembles_val_loss[model_type].sort_values(ascending=True).index[:selected_models]\n",
    "    models_dict[model_type]['best_ensemble'] = [models_dict[model_type]['ensemble'][index] for index in selected_indices]\n",
    "\n",
    "def selected_ensemble_predict(models_dict, model_type, X_unseen, Y_unseen, test_df, selected_models):\n",
    "    '''\n",
    "    This function will predict the unseen data using the selected models and calculate the error.\n",
    "    It will store the results in the test_df['BET'] column.\n",
    "    It will also use the logistic adjustment to make the adjusted prediction and then calculate the error.\n",
    "    It will store the adjusted results in the test_df['BETA'] column.\n",
    "    '''\n",
    "\n",
    "\n",
    "    ensemble_pred = [model.predict(X_unseen) for model in models_dict[model_type]['best_ensemble']]\n",
    "    test_pred = np.mean(ensemble_pred[:selected_models], axis=0)\n",
    "    test_df.loc['BET', model_type] = mean_absolute_error(Y_unseen, test_pred)\n",
    "\n",
    "    test_pred_adjusted = np.array([soft_max_adjustnent(value) for value in test_pred])\n",
    "    test_df.loc['BETA', model_type] = mean_absolute_error(Y_unseen, test_pred_adjusted)\n",
    "\n",
    "    return test_pred_adjusted\n",
    "\n",
    "selected_models = 5\n",
    "\n",
    "#-------------------PIPE 1-------------------\n",
    "for model_type in ['KNN', 'SGD', 'Tree']:\n",
    "    select_best_models(models_dict_1, Ensembles_val_loss_1, model_type, selected_models)\n",
    "\n",
    "Y_unseen_pred_KNN_1_adjusted = selected_ensemble_predict(models_dict_1, 'KNN', X_unseen_1, Y_unseen_1, test_df_1, selected_models)\n",
    "Y_unseen_pred_SGD_1_adjusted = selected_ensemble_predict(models_dict_1, 'SGD', X_unseen_1, Y_unseen_1, test_df_1, selected_models)\n",
    "Y_unseen_pred_Tree_1_adjusted = selected_ensemble_predict(models_dict_1, 'Tree', X_unseen_1, Y_unseen_1, test_df_1, selected_models)\n",
    "\n",
    "plot_histograms(Y_unseen_1, Y_unseen_pred_KNN_1_adjusted, Y_unseen_pred_SGD_1_adjusted, Y_unseen_pred_Tree_1_adjusted)\n",
    "\n",
    "\n",
    "#-------------------PIPE 2-------------------\n",
    "for model_type in ['KNN', 'SGD', 'Tree']:\n",
    "    select_best_models(models_dict_2, Ensembles_val_loss_2, model_type, selected_models)\n",
    "\n",
    "Y_unseen_pred_KNN_2_adjusted = selected_ensemble_predict(models_dict_2, 'KNN', X_unseen_2, Y_unseen_2, test_df_2, selected_models)\n",
    "Y_unseen_pred_SGD_2_adjusted = selected_ensemble_predict(models_dict_2, 'SGD', X_unseen_2, Y_unseen_2, test_df_2, selected_models)\n",
    "Y_unseen_pred_Tree_2_adjusted = selected_ensemble_predict(models_dict_2, 'Tree', X_unseen_2, Y_unseen_2, test_df_2, selected_models)\n",
    "\n",
    "plot_histograms(Y_unseen_2, Y_unseen_pred_KNN_2_adjusted, Y_unseen_pred_SGD_2_adjusted, Y_unseen_pred_Tree_2_adjusted)\n",
    "\n",
    "display(pd.concat([test_df_1,test_df_2],axis=1)*Y_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"orange\">  **Autograder** </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_autograder)\n",
    "\n",
    "estimate_MAE_on_new_data = np.array(test_df_2['SGD']['BETA']) * 18.707 # Y_2_std\n",
    "print(f\"Estimated MAE on new data: {estimate_MAE_on_new_data}\")\n",
    "\n",
    "predictions_autograder_data = np.array([model.predict(df_autograder) for model in models_dict_1['SGD']['best_ensemble']]).mean(axis=0)\n",
    "\n",
    "# # Upload this file to the Vocareum autograder:\n",
    "result = np.append(estimate_MAE_on_new_data, predictions_autograder_data)\n",
    "pd.DataFrame(result).to_csv(\"autograder_submission.txt\", index=False, header=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env1Conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
