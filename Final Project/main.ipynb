{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-23T22:19:17.827001300Z",
     "start_time": "2024-10-23T22:19:17.784922800Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import pprint\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "pd.options.display.float_format = '{:.3f}'.format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"orange\">  **Data Loading and Visualizaiton** </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-23T22:26:13.887707300Z",
     "start_time": "2024-10-23T22:26:13.807773500Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "    whrswk  hhi  whi hhi2   education   race hispanic  experience  kidslt6  \\\n8    5.000  yes   no  yes  13-15years    NaN       no      20.000    1.000   \n9   40.000   no  yes  yes     12years  white       no       9.000    1.000   \n10  40.000   no   no   no     12years  white       no      33.000    0.000   \n11  40.000   no   no   no   9-11years  white       no       5.500    1.000   \n12  35.000   no  yes   no     12years    NaN      yes      26.000    0.000   \n13  45.000   no  yes  yes     16years  white       no       7.000    0.000   \n14   0.000  yes   no  yes     12years  white       no      38.000    0.000   \n15  20.000  yes   no  yes     12years  white       no       9.000    1.000   \n\n    kids618  husby        region  \n8     1.000 53.000         south  \n9     1.000 18.000          west  \n10    0.000  0.000         other  \n11    0.000 20.000  northcentral  \n12    1.000  0.000         south  \n13    0.000 40.000         south  \n14    0.000  0.000         other  \n15    1.000 33.000         south  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>whrswk</th>\n      <th>hhi</th>\n      <th>whi</th>\n      <th>hhi2</th>\n      <th>education</th>\n      <th>race</th>\n      <th>hispanic</th>\n      <th>experience</th>\n      <th>kidslt6</th>\n      <th>kids618</th>\n      <th>husby</th>\n      <th>region</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>8</th>\n      <td>5.000</td>\n      <td>yes</td>\n      <td>no</td>\n      <td>yes</td>\n      <td>13-15years</td>\n      <td>NaN</td>\n      <td>no</td>\n      <td>20.000</td>\n      <td>1.000</td>\n      <td>1.000</td>\n      <td>53.000</td>\n      <td>south</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>40.000</td>\n      <td>no</td>\n      <td>yes</td>\n      <td>yes</td>\n      <td>12years</td>\n      <td>white</td>\n      <td>no</td>\n      <td>9.000</td>\n      <td>1.000</td>\n      <td>1.000</td>\n      <td>18.000</td>\n      <td>west</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>40.000</td>\n      <td>no</td>\n      <td>no</td>\n      <td>no</td>\n      <td>12years</td>\n      <td>white</td>\n      <td>no</td>\n      <td>33.000</td>\n      <td>0.000</td>\n      <td>0.000</td>\n      <td>0.000</td>\n      <td>other</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>40.000</td>\n      <td>no</td>\n      <td>no</td>\n      <td>no</td>\n      <td>9-11years</td>\n      <td>white</td>\n      <td>no</td>\n      <td>5.500</td>\n      <td>1.000</td>\n      <td>0.000</td>\n      <td>20.000</td>\n      <td>northcentral</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>35.000</td>\n      <td>no</td>\n      <td>yes</td>\n      <td>no</td>\n      <td>12years</td>\n      <td>NaN</td>\n      <td>yes</td>\n      <td>26.000</td>\n      <td>0.000</td>\n      <td>1.000</td>\n      <td>0.000</td>\n      <td>south</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>45.000</td>\n      <td>no</td>\n      <td>yes</td>\n      <td>yes</td>\n      <td>16years</td>\n      <td>white</td>\n      <td>no</td>\n      <td>7.000</td>\n      <td>0.000</td>\n      <td>0.000</td>\n      <td>40.000</td>\n      <td>south</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>0.000</td>\n      <td>yes</td>\n      <td>no</td>\n      <td>yes</td>\n      <td>12years</td>\n      <td>white</td>\n      <td>no</td>\n      <td>38.000</td>\n      <td>0.000</td>\n      <td>0.000</td>\n      <td>0.000</td>\n      <td>other</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>20.000</td>\n      <td>yes</td>\n      <td>no</td>\n      <td>yes</td>\n      <td>12years</td>\n      <td>white</td>\n      <td>no</td>\n      <td>9.000</td>\n      <td>1.000</td>\n      <td>1.000</td>\n      <td>33.000</td>\n      <td>south</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "    hhi  whi hhi2   education   race hispanic  experience  kidslt6  kids618  \\\n8   yes  yes  yes  13-15years  white       no      35.000    0.000    0.000   \n9    no  yes   no     16years  white       no       1.000    0.000    0.000   \n10   no   no   no     12years  white       no      14.000      NaN      NaN   \n11   no  yes   no     12years  white       no      44.000      NaN      NaN   \n12   no  yes  yes  13-15years  white       no      23.000    0.000    3.000   \n13   no  yes   no     16years  white       no      10.000    0.000    0.000   \n14  yes   no  yes    >16years  white       no      23.000    0.000    0.000   \n15  yes   no  yes     12years  white       no      22.000    1.000    0.000   \n\n    husby        region  \n8  52.861         south  \n9   5.000  northcentral  \n10  0.000  northcentral  \n11  0.000          west  \n12 45.000          west  \n13  0.000         south  \n14 40.100          west  \n15 24.846         other  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>hhi</th>\n      <th>whi</th>\n      <th>hhi2</th>\n      <th>education</th>\n      <th>race</th>\n      <th>hispanic</th>\n      <th>experience</th>\n      <th>kidslt6</th>\n      <th>kids618</th>\n      <th>husby</th>\n      <th>region</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>8</th>\n      <td>yes</td>\n      <td>yes</td>\n      <td>yes</td>\n      <td>13-15years</td>\n      <td>white</td>\n      <td>no</td>\n      <td>35.000</td>\n      <td>0.000</td>\n      <td>0.000</td>\n      <td>52.861</td>\n      <td>south</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>no</td>\n      <td>yes</td>\n      <td>no</td>\n      <td>16years</td>\n      <td>white</td>\n      <td>no</td>\n      <td>1.000</td>\n      <td>0.000</td>\n      <td>0.000</td>\n      <td>5.000</td>\n      <td>northcentral</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>no</td>\n      <td>no</td>\n      <td>no</td>\n      <td>12years</td>\n      <td>white</td>\n      <td>no</td>\n      <td>14.000</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.000</td>\n      <td>northcentral</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>no</td>\n      <td>yes</td>\n      <td>no</td>\n      <td>12years</td>\n      <td>white</td>\n      <td>no</td>\n      <td>44.000</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.000</td>\n      <td>west</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>no</td>\n      <td>yes</td>\n      <td>yes</td>\n      <td>13-15years</td>\n      <td>white</td>\n      <td>no</td>\n      <td>23.000</td>\n      <td>0.000</td>\n      <td>3.000</td>\n      <td>45.000</td>\n      <td>west</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>no</td>\n      <td>yes</td>\n      <td>no</td>\n      <td>16years</td>\n      <td>white</td>\n      <td>no</td>\n      <td>10.000</td>\n      <td>0.000</td>\n      <td>0.000</td>\n      <td>0.000</td>\n      <td>south</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>yes</td>\n      <td>no</td>\n      <td>yes</td>\n      <td>&gt;16years</td>\n      <td>white</td>\n      <td>no</td>\n      <td>23.000</td>\n      <td>0.000</td>\n      <td>0.000</td>\n      <td>40.100</td>\n      <td>west</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>yes</td>\n      <td>no</td>\n      <td>yes</td>\n      <td>12years</td>\n      <td>white</td>\n      <td>no</td>\n      <td>22.000</td>\n      <td>1.000</td>\n      <td>0.000</td>\n      <td>24.846</td>\n      <td>other</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.read_csv('health_insurance_train.csv')\n",
    "df_autograder = pd.read_csv('health_insurance_autograde.csv')\n",
    "                            \n",
    "display(df.iloc[8:16])\n",
    "display(df_autograder.iloc[8:16])"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def preprocess_1(df):\n",
    "    # turns all data numeric except regions\n",
    "    education_mapping = {\n",
    "        '<9years': 8,      # Assume '<9years' corresponds to 8 years\n",
    "        '9-11years': 10,   # Midpoint for '9-11years'\n",
    "        '12years': 12,     # Exact number of years\n",
    "        '11-13years': 12,  # Midpoint for '11-13years'\n",
    "        '13-15years': 14,  # Midpoint for '13-15years'\n",
    "        '16years': 16,     # Exact number of years\n",
    "        '>16years': 18     # Assume '>16years' corresponds to 17 years\n",
    "    }\n",
    "    \n",
    "    yn_mapping = {'yes': 1, 'no': 0}\n",
    "    race_mapping = {'white': 1, 'black': 0}\n",
    "    \n",
    "    df['education'] = df['education'].map(education_mapping)\n",
    "    df['race'] = df['race'].map(race_mapping).fillna(0.5)\n",
    "    \n",
    "    binary_columns = ['hhi', 'whi', 'hhi2', 'hispanic']\n",
    "    for col in binary_columns:\n",
    "        df[col] = df[col].map(yn_mapping)\n",
    "    \n",
    "    df['kidslt6'] = df['kidslt6'].fillna(df['kidslt6'].median())\n",
    "    df['kids618'] = df['kids618'].fillna(df['kids618'].median())\n",
    "    \n",
    "    \n",
    "    display(df.iloc[8:16])\n",
    "    return df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-23T22:26:15.340078800Z",
     "start_time": "2024-10-23T22:26:15.269097500Z"
    }
   },
   "execution_count": 75
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def visualize_data(df):\n",
    "    # 1. Histograms\n",
    "    df.hist(figsize=(12, 8), bins=10, color='skyblue', edgecolor='black')\n",
    "    plt.suptitle('Histograms of dfset Features')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # 2. Pairplot - Showing pairwise relationships\n",
    "    # sns.pairplot(df)\n",
    "    # plt.suptitle('Pairwise Plot of Features')\n",
    "    # plt.show()\n",
    "    \n",
    "    # 3. Correlation heatmap\n",
    "    data = df.drop('region', axis=1)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.heatmap(data.corr(), annot=True, cmap='coolwarm', linewidths=0.5)\n",
    "    plt.title('Correlation Heatmap of Features')\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "    whrswk  hhi  whi  hhi2  education  race  hispanic  experience  kidslt6  \\\n8    5.000    1    0     1         14 0.500         0      20.000    1.000   \n9   40.000    0    1     1         12 1.000         0       9.000    1.000   \n10  40.000    0    0     0         12 1.000         0      33.000    0.000   \n11  40.000    0    0     0         10 1.000         0       5.500    1.000   \n12  35.000    0    1     0         12 0.500         1      26.000    0.000   \n13  45.000    0    1     1         16 1.000         0       7.000    0.000   \n14   0.000    1    0     1         12 1.000         0      38.000    0.000   \n15  20.000    1    0     1         12 1.000         0       9.000    1.000   \n\n    kids618  husby        region  \n8     1.000 53.000         south  \n9     1.000 18.000          west  \n10    0.000  0.000         other  \n11    0.000 20.000  northcentral  \n12    1.000  0.000         south  \n13    0.000 40.000         south  \n14    0.000  0.000         other  \n15    1.000 33.000         south  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>whrswk</th>\n      <th>hhi</th>\n      <th>whi</th>\n      <th>hhi2</th>\n      <th>education</th>\n      <th>race</th>\n      <th>hispanic</th>\n      <th>experience</th>\n      <th>kidslt6</th>\n      <th>kids618</th>\n      <th>husby</th>\n      <th>region</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>8</th>\n      <td>5.000</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>14</td>\n      <td>0.500</td>\n      <td>0</td>\n      <td>20.000</td>\n      <td>1.000</td>\n      <td>1.000</td>\n      <td>53.000</td>\n      <td>south</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>40.000</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>12</td>\n      <td>1.000</td>\n      <td>0</td>\n      <td>9.000</td>\n      <td>1.000</td>\n      <td>1.000</td>\n      <td>18.000</td>\n      <td>west</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>40.000</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>12</td>\n      <td>1.000</td>\n      <td>0</td>\n      <td>33.000</td>\n      <td>0.000</td>\n      <td>0.000</td>\n      <td>0.000</td>\n      <td>other</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>40.000</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>10</td>\n      <td>1.000</td>\n      <td>0</td>\n      <td>5.500</td>\n      <td>1.000</td>\n      <td>0.000</td>\n      <td>20.000</td>\n      <td>northcentral</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>35.000</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>12</td>\n      <td>0.500</td>\n      <td>1</td>\n      <td>26.000</td>\n      <td>0.000</td>\n      <td>1.000</td>\n      <td>0.000</td>\n      <td>south</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>45.000</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>16</td>\n      <td>1.000</td>\n      <td>0</td>\n      <td>7.000</td>\n      <td>0.000</td>\n      <td>0.000</td>\n      <td>40.000</td>\n      <td>south</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>0.000</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>12</td>\n      <td>1.000</td>\n      <td>0</td>\n      <td>38.000</td>\n      <td>0.000</td>\n      <td>0.000</td>\n      <td>0.000</td>\n      <td>other</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>20.000</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>12</td>\n      <td>1.000</td>\n      <td>0</td>\n      <td>9.000</td>\n      <td>1.000</td>\n      <td>1.000</td>\n      <td>33.000</td>\n      <td>south</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = preprocess_1(df)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-23T22:26:18.004157800Z",
     "start_time": "2024-10-23T22:26:17.934938800Z"
    }
   },
   "execution_count": 76
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df.info()"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "visualize_data(df)"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df_autograder = preprocess_1(df_autograder)"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n",
    "# race_counts = df['race'].value_counts()\n",
    "# race_counts_autograder = df_autograder['race'].value_counts()\n",
    "# print(race_counts, race_counts_autograder)"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''''\n",
    "* Data visualization (Edlyn)\n",
    "- Make nice histograms of the features and the target\n",
    "- Make a covariance matrix of the features\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"orange\">  **Data processing fucntions** </font>"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "   whrswk  hhi  whi  hhi2  education  race  hispanic  experience  kidslt6  \\\n0  40.000    0    1     1         14 1.000         0      17.000    0.000   \n1  40.000    0    1     1         14 1.000         0       4.000    1.000   \n2   0.000    1    0     1         16 1.000         0      21.000    0.000   \n3  40.000    0    0     1         14 1.000         0      22.000    0.000   \n4  35.000    0    1     0         12 1.000         0      15.000    0.000   \n\n   kids618  husby  reg_other  reg_south  reg_west  \n0    1.000 22.000          0          1         0  \n1    0.000 15.000          0          1         0  \n2    1.000 99.999          1          0         0  \n3    0.000 60.000          0          0         0  \n4    2.000  0.000          0          1         0  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>whrswk</th>\n      <th>hhi</th>\n      <th>whi</th>\n      <th>hhi2</th>\n      <th>education</th>\n      <th>race</th>\n      <th>hispanic</th>\n      <th>experience</th>\n      <th>kidslt6</th>\n      <th>kids618</th>\n      <th>husby</th>\n      <th>reg_other</th>\n      <th>reg_south</th>\n      <th>reg_west</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>40.000</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>14</td>\n      <td>1.000</td>\n      <td>0</td>\n      <td>17.000</td>\n      <td>0.000</td>\n      <td>1.000</td>\n      <td>22.000</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>40.000</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>14</td>\n      <td>1.000</td>\n      <td>0</td>\n      <td>4.000</td>\n      <td>1.000</td>\n      <td>0.000</td>\n      <td>15.000</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.000</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>16</td>\n      <td>1.000</td>\n      <td>0</td>\n      <td>21.000</td>\n      <td>0.000</td>\n      <td>1.000</td>\n      <td>99.999</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>40.000</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>14</td>\n      <td>1.000</td>\n      <td>0</td>\n      <td>22.000</td>\n      <td>0.000</td>\n      <td>0.000</td>\n      <td>60.000</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>35.000</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>12</td>\n      <td>1.000</td>\n      <td>0</td>\n      <td>15.000</td>\n      <td>0.000</td>\n      <td>2.000</td>\n      <td>0.000</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# One hot encoding\n",
    "def OneHotEncode(df):\n",
    "    df = pd.get_dummies(df, columns=['region'],prefix='reg', drop_first=True)\n",
    "    tf_mapping = {True: 1, False: 0}\n",
    "    cols = ['reg_other', 'reg_south', 'reg_west', 'reg_northcentral']\n",
    "    for col in cols:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].map(tf_mapping)\n",
    "    return df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-23T22:26:58.059883400Z",
     "start_time": "2024-10-23T22:26:58.030108400Z"
    }
   },
   "execution_count": 77
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "preprocessor_standard = ColumnTransformer(transformers=[\n",
    "    ('num', Pipeline(steps=[\n",
    "        ('scaler', StandardScaler())\n",
    "    ]), ['experience', 'kidslt6', 'kids618', 'husby', 'education']),\n",
    "])\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''''\n",
    "* Create the data pipeline functions (Edlyn)\n",
    "\n",
    "- Preprocess data (PIPE 1, PIPE 2)\n",
    "    . Apply one hot encoding for region, race and hispanic. Make a column for race_nan and hispanic_nan. \n",
    "      People that decide to not fill these information might have a characteristic profile. So we can let the\n",
    "      model decide if it is important or not.\n",
    "\n",
    "    . Make remaining yes/no 1 and -1 (helps KNN, since the features are logically opoosite of each other)\n",
    "\n",
    "    . Make True/False 1 and 0 (When it is not just about \"yes/no\" give 1 to True and 0 to False)\n",
    "\n",
    "    . Process education column, nan becomes average \n",
    "\n",
    "    . You can apply remove first if you want\n",
    "\n",
    "\n",
    "- Regular scaling (PIPE 1) \n",
    "\n",
    "- Selective scaling (PIPE 2)\n",
    "\n",
    "- Data filtering with mahalanobis (PIPE 2)\n",
    "\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"orange\">  **Pipeline creation** </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = pd.read_csv('health_insurance_train_processed.csv')# Use the pipiline 1 function function instead of this file\n",
    "\n",
    "display(df_1.iloc[9:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2 = pd.read_csv('health_insurance_train_processed.csv') # Use the pipiline 2 function instead of this file\n",
    "\n",
    "display(df_2.iloc[9:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into seen and unseen while keeping it as a pandas dataframe\n",
    "fraction = 0.2  # 20% of the rows\n",
    "\n",
    "#-------------------PIPE 1-------------------\n",
    "df_unseen_1 = df_1.sample(frac = fraction, random_state=42) # Get 20% of random rows\n",
    "df_seen_1 = df_1.drop(df_unseen_1.index) # Get the remaining 80% of the rows\n",
    "\n",
    "X_seen_1 = df_seen_1.iloc[:, 1:]\n",
    "Y_seen_1 = df_seen_1.iloc[:, 0]\n",
    "\n",
    "X_unseen_1 = df_unseen_1.iloc[:, 1:]\n",
    "Y_unseen_1 = df_unseen_1.iloc[:, 0]\n",
    "\n",
    "#-------------------PIPE 2-------------------\n",
    "df_unseen_2 = df_2.sample(frac = fraction, random_state=42)\n",
    "df_seen_2 = df_2.drop(df_unseen_2.index)\n",
    "\n",
    "X_seen_2 = df_seen_2.iloc[:, 1:]\n",
    "Y_seen_2 = df_seen_2.iloc[:, 0]\n",
    "\n",
    "X_unseen_2 = df_unseen_2.iloc[:, 1:]\n",
    "Y_unseen_2 = df_unseen_2.iloc[:, 0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"orange\">  **Training Functions** </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#-------------------Dummy-------------------\n",
    "from sklearn.dummy import DummyRegressor\n",
    "def train_dummy_predictor(X, Y):\n",
    "    model = DummyRegressor(strategy='mean')\n",
    "    model.fit(X, Y)\n",
    "    return model\n",
    "\n",
    "#-------------------KNN-------------------\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "def train_knn_regressor(X, Y, param_grid):\n",
    "    model = KNeighborsRegressor(**param_grid)\n",
    "    model.fit(X, Y)\n",
    "    Y_pred = model.predict(X)\n",
    "    loss_values = [mean_absolute_error(Y, Y_pred)]\n",
    "    return model,loss_values\n",
    "\n",
    "#-------------------SGD-------------------\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "def train_sgd_regressor(X, Y, params):\n",
    "    model = SGDRegressor(**params)\n",
    "    epochs = params['max_iter']\n",
    "\n",
    "    loss_values = []\n",
    "    for epoch in range(epochs):\n",
    "        model.partial_fit(X, Y)\n",
    "        Y_pred = model.predict(X)\n",
    "        epoch_loss = mean_absolute_error(Y, Y_pred)\n",
    "        loss_values.append(epoch_loss)\n",
    "    \n",
    "    return model, loss_values\n",
    "\n",
    "#-----------Decision Tree-------------------\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "def train_decision_tree_regressor(X, Y, params):\n",
    "\n",
    "    '''params : dict\n",
    "        Dictionary of parameters to pass to DecisionTreeRegressor.'''\n",
    "    \n",
    "    # splitter = Supported strategies are \"best\" to choose the best split and \"random\" to choose the best random split.\n",
    "    # max_features = The number of features to consider when looking for the best split\n",
    "    # min_samples_split = The minimum number of samples required to split an internal node\n",
    "    # min_samples_leaf = The minimum number of samples required to be at a leaf node\n",
    "\n",
    "    model = DecisionTreeRegressor(**params,random_state = 42)\n",
    "    loss_values = []\n",
    "    \n",
    "    # Custom training loop with logging\n",
    "    for depth in range(1, params['max_depth'] + 1):\n",
    "        model.set_params(max_depth=depth)\n",
    "        model.fit(X, Y)\n",
    "        Y_pred = model.predict(X)\n",
    "        loss = mean_absolute_error(Y, Y_pred)\n",
    "        loss_values.append(loss)\n",
    "    \n",
    "    return model, loss_values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"orange\">  **Logistic Adjustment** </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def soft_max_adjustnent(value, target1 = -1.372319745743416, target2= 0.765866095571318 , gamma=5):\n",
    "    # Calculate the distance to each target\n",
    "    dist_to_target1 = abs(value - target1)\n",
    "    dist_to_target2 = abs(value - target2)\n",
    "    \n",
    "    # Transform the distance to probability\n",
    "    # Assumed the distance follows a Exponential distribution\n",
    "    prob_target1 = np.exp(-gamma * dist_to_target1)\n",
    "    prob_target2 = np.exp(-gamma * dist_to_target2)\n",
    "    \n",
    "    # Normalize the probabilities using soft max\n",
    "    total_prob = prob_target1 + prob_target2\n",
    "    prob_target1 /= total_prob\n",
    "    prob_target2 /= total_prob\n",
    "    \n",
    "    # Adjust the value based on the probabilities\n",
    "    adjusted_value = prob_target1 * target1 + prob_target2 * target2\n",
    "    \n",
    "    return adjusted_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"orange\">  **Create model dict and test dataframe** </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#----------Creation of models dict-----------\n",
    "models_dict_1 = {'KNN': None, 'SGD': None, 'Tree':None}\n",
    "models_dict_2 = {'KNN': None, 'SGD': None, 'Tree':None}\n",
    "\n",
    "for key in models_dict_1:\n",
    "    models_dict_1[key] = {'defalt' :None, 'best_param':None, 'best_model' :None, 'ensemble':None, 'best_ensemble':None}\n",
    "    models_dict_2[key] = {'defalt' :None, 'best_param':None, 'best_model' :None, 'ensemble':None, 'best_ensemble':None}\n",
    "\n",
    "#----------Creation of test dfs-----------\n",
    "test_df_1 = pd.DataFrame(index=['D','T','TA','ET','BET','BETA'],columns=['KNN','SGD','Tree'])\n",
    "test_df_2 = pd.DataFrame(index=['D','T','TA','ET','BET','BETA'],columns=['KNN','SGD','Tree'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"orange\">  **Store and test default models** </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################\n",
    "# ------ KNN REGRESSOR\n",
    "print(\"------ KNN Regressor -----\")\n",
    "model = KNeighborsRegressor()\n",
    "model.fit(X_seen_1, Y_seen_1)\n",
    "models_dict_1['KNN']['defalt'] = model\n",
    "test_df_1.loc['D','KNN'] = mean_absolute_error(Y_unseen_1, model.predict(X_unseen_1))\n",
    "\n",
    "model = KNeighborsRegressor()\n",
    "model.fit(X_seen_2, Y_seen_2)\n",
    "models_dict_2['KNN']['defalt'] = model\n",
    "test_df_2.loc['D','KNN'] = mean_absolute_error(Y_unseen_2, model.predict(X_unseen_2))\n",
    "\n",
    "\n",
    "####################################################################\n",
    "# ------ SGD REGRESSOR\n",
    "print(\"------ SGD Regressor -----\")\n",
    "model = SGDRegressor()\n",
    "model.fit(X_seen_1, Y_seen_1)\n",
    "models_dict_1['SGD']['defalt'] = model\n",
    "test_df_1.loc['D','SGD'] = mean_absolute_error(Y_unseen_1, model.predict(X_unseen_1))\n",
    "\n",
    "model = SGDRegressor()\n",
    "model.fit(X_seen_2,  Y_seen_2)\n",
    "models_dict_2['SGD']['defalt'] = model\n",
    "test_df_2.loc['D','SGD'] = mean_absolute_error(Y_unseen_2, model.predict(X_unseen_2))\n",
    "\n",
    "####################################################################\n",
    "# ------ DECISION TREE REGRESSOR\n",
    "print(\"------ Decision Tree Regressor -----\")\n",
    "model = DecisionTreeRegressor()\n",
    "model.fit(X_seen_1, Y_seen_1)\n",
    "models_dict_1['Tree']['defalt'] = model\n",
    "test_df_1.loc['D','Tree'] = mean_absolute_error(Y_unseen_1, model.predict(X_unseen_1))\n",
    "\n",
    "model = DecisionTreeRegressor()\n",
    "model.fit(X_seen_2, Y_seen_2)\n",
    "models_dict_2['Tree']['defalt'] = model\n",
    "test_df_2.loc['D','Tree'] = mean_absolute_error(Y_unseen_2, model.predict(X_unseen_2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"orange\">  **Perform grid search, test, store best models and parameters** </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def grid_search(X, Y, model, param_grid, cv=5):\n",
    "    \n",
    "    # cv = It determines the cross-validation splitting strategy used to evaluate the performance of the model for each combination of hyperparameters\n",
    "    # This means that the dataset will be split into 5 parts (folds). The model will be trained on 4 parts and tested on the remaining part.\n",
    "    # This process will be repeated 5 times, each time with a different part as the test set.\n",
    "    grid_search = GridSearchCV(estimator=model, param_grid=param_grid, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1)\n",
    "    \n",
    "    # Fit the model\n",
    "    print(\"Working on grid search\")\n",
    "    grid_search.fit(X, Y)\n",
    "    \n",
    "    # Get the best model and parameters\n",
    "    best_model = grid_search.best_estimator_\n",
    "    best_params = grid_search.best_params_\n",
    "    \n",
    "    print(f\"Best parameters: {best_params}\\n\")\n",
    "    \n",
    "    return best_model, best_params\n",
    "\n",
    "\n",
    "def search_train_test_store(model, param_grid, X_seen, Y_seen, X_unseen, Y_unseen, models_dict, model_name, test_df):\n",
    "    # Perform grid search\n",
    "    best_model, best_params = grid_search(X_seen, Y_seen, model, param_grid, cv=5)\n",
    "    models_dict[model_name]['best_model'] = best_model\n",
    "    models_dict[model_name]['best_param'] = best_params\n",
    "    \n",
    "    # Predict and calculate errors\n",
    "    Y_pred = best_model.predict(X_unseen)\n",
    "    test_df.loc['T', model_name] = mean_absolute_error(Y_unseen, Y_pred)\n",
    "    Y_pred_adjusted = np.array([soft_max_adjustnent(value) for value in Y_pred])\n",
    "    test_df.loc['TA', model_name] = mean_absolute_error(Y_unseen, Y_pred_adjusted)\n",
    "\n",
    "# Define parameter grids\n",
    "param_grid_KNN = {\n",
    "    'n_neighbors': [3, 5, 7, 9, 11, 13, 15, 17, 19, 21],\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'p': [1, 2]  # 1 for Manhattan distance, 2 for Euclidean distance\n",
    "}\n",
    "\n",
    "param_grid_Tree = {\n",
    "    'criterion': ['squared_error', 'friedman_mse', 'absolute_error'],\n",
    "    'splitter': ['random'],\n",
    "    'max_depth': [10, 15, 20, 25],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4, 8],\n",
    "    'max_features': [None, 'sqrt', 'log2']\n",
    "}\n",
    "\n",
    "param_grid_SGD = {\n",
    "    'alpha': [0.0001, 0.001, 0.01, 0.1],\n",
    "    'penalty': ['l2', 'l1', 'elasticnet'],\n",
    "    'learning_rate': ['constant', 'optimal', 'invscaling', 'adaptive'],\n",
    "    'eta0': [0.00001, 0.0001],\n",
    "    'max_iter': [50, 100, 200]\n",
    "}\n",
    "\n",
    "# Train and evaluate models\n",
    "print(\"------ KNN Regressor -----\")\n",
    "search_train_test_store(KNeighborsRegressor(), param_grid_KNN, X_seen_1, Y_seen_1, X_unseen_1, Y_unseen_1, models_dict_1, 'KNN', test_df_1)\n",
    "search_train_test_store(KNeighborsRegressor(), param_grid_KNN, X_seen_2, Y_seen_2, X_unseen_2, Y_unseen_2, models_dict_2, 'KNN', test_df_2)\n",
    "\n",
    "print(\"------ Decision Tree Regressor -----\")\n",
    "search_train_test_store(DecisionTreeRegressor(random_state=42), param_grid_Tree, X_seen_1, Y_seen_1, X_unseen_1, Y_unseen_1, models_dict_1, 'Tree', test_df_1)\n",
    "search_train_test_store(DecisionTreeRegressor(random_state=42), param_grid_Tree, X_seen_2, Y_seen_2, X_unseen_2, Y_unseen_2, models_dict_2, 'Tree', test_df_2)\n",
    "\n",
    "print(\"------ SGD Regressor -----\")\n",
    "search_train_test_store(SGDRegressor(random_state=42), param_grid_SGD, X_seen_1, Y_seen_1, X_unseen_1, Y_unseen_1, models_dict_1, 'SGD', test_df_1)\n",
    "search_train_test_store(SGDRegressor(random_state=42), param_grid_SGD, X_seen_2, Y_seen_2, X_unseen_2, Y_unseen_2, models_dict_2, 'SGD', test_df_2)\n",
    "\n",
    "# Display results\n",
    "display(test_df_1)\n",
    "display(test_df_2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"orange\">  **Ensemble training and validation tests** </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_models(X_seen,Y_seen,params, model_train_function, n_models = 10, val_size = 0.2):\n",
    "\n",
    "    models_training_loss = []\n",
    "    models_val_loss = []\n",
    "    model_list = []\n",
    "\n",
    "\n",
    "    for n in range(n_models):\n",
    "        X_train, X_val, Y_train, Y_val = train_test_split(X_seen, Y_seen, test_size= val_size, random_state= 42*n)\n",
    "\n",
    "        model, loss_values = model_train_function(X_train, Y_train, params)\n",
    "        \n",
    "        model_list.append(model)\n",
    "        models_training_loss.append(loss_values)\n",
    "\n",
    "        Y_pred = model.predict(X_val)\n",
    "        val_loss = mean_absolute_error(Y_val, Y_pred)\n",
    "        models_val_loss.append(val_loss)\n",
    "    \n",
    "    return model_list, models_training_loss, models_val_loss\n",
    "\n",
    "n_models = 30\n",
    "\n",
    "# Create a dataframe to store the validation loss of each model with the mean at the end\n",
    "Ensemble_val_loss = pd.DataFrame(index=range(n_models + 1))\n",
    "Ensemble_val_loss.rename(index={n_models: 'mean'}, inplace=True)\n",
    "\n",
    "print(Ensemble_val_loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env1Conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
