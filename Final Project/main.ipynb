{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import pprint\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "pd.options.display.float_format = '{:.3f}'.format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"orange\">  **Data Loading and Visualizaiton** </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('health_insurance_train.csv')\n",
    "df_autograder = pd.read_csv('health_insurance_autograde.csv')\n",
    "                            \n",
    "display(df.iloc[8:16])\n",
    "display(df_autograder.iloc[8:16])"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def preprocess_1(df):\n",
    "    # turns all data numeric except regions\n",
    "    education_mapping = {\n",
    "        '<9years': 8,      # Assume '<9years' corresponds to 8 years\n",
    "        '9-11years': 10,   # Midpoint for '9-11years'\n",
    "        '12years': 12,     # Exact number of years\n",
    "        '11-13years': 12,  # Midpoint for '11-13years'\n",
    "        '13-15years': 14,  # Midpoint for '13-15years'\n",
    "        '16years': 16,     # Exact number of years\n",
    "        '>16years': 18     # Assume '>16years' corresponds to 17 years\n",
    "    }\n",
    "    \n",
    "    yn_mapping = {'yes': 1, 'no': 0}\n",
    "    race_mapping = {'white': 1, 'black': 0}\n",
    "    \n",
    "    df['education'] = df['education'].map(education_mapping)\n",
    "    df['race'] = df['race'].map(race_mapping).fillna(0.5)\n",
    "    \n",
    "    binary_columns = ['hhi', 'whi', 'hhi2', 'hispanic']\n",
    "    for col in binary_columns:\n",
    "        df[col] = df[col].map(yn_mapping)\n",
    "    \n",
    "    df['kidslt6'] = df['kidslt6'].fillna(df['kidslt6'].median())\n",
    "    df['kids618'] = df['kids618'].fillna(df['kids618'].median())\n",
    "    \n",
    "    \n",
    "    display(df.iloc[8:16])\n",
    "    return df"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def visualize_data(df):\n",
    "    # 1. Histograms\n",
    "    df.hist(figsize=(12, 8), bins=10, color='skyblue', edgecolor='black')\n",
    "    plt.suptitle('Histograms of dfset Features')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # 2. Pairplot - Showing pairwise relationships\n",
    "    # sns.pairplot(df)\n",
    "    # plt.suptitle('Pairwise Plot of Features')\n",
    "    # plt.show()\n",
    "    \n",
    "    # 3. Correlation heatmap\n",
    "    if 'region' in df.columns:\n",
    "        data = df.drop('region', axis=1)\n",
    "    else:\n",
    "        data = df.copy()\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.heatmap(data.corr(), annot=True, cmap='coolwarm', linewidths=0.5)\n",
    "    plt.title('Correlation Heatmap of Features')\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df = preprocess_1(df)"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df.info()"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "visualize_data(df)"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df_autograder = preprocess_1(df_autograder)"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n",
    "# race_counts = df['race'].value_counts()\n",
    "# race_counts_autograder = df_autograder['race'].value_counts()\n",
    "# print(race_counts, race_counts_autograder)"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''''\n",
    "* Data visualization (Edlyn)\n",
    "- Make nice histograms of the features and the target\n",
    "- Make a covariance matrix of the features\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"orange\">  **Data processing fucntions** </font>"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# One hot encoding\n",
    "def onehotencode(df):\n",
    "    df = pd.get_dummies(df, columns=['region'],prefix='reg', drop_first=True)\n",
    "    tf_mapping = {True: 1, False: 0}\n",
    "    cols = ['reg_other', 'reg_south', 'reg_west', 'reg_northcentral']\n",
    "    for col in cols:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].map(tf_mapping)\n",
    "    return df"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df = onehotencode(df)\n",
    "df_autograder = onehotencode(df_autograder)"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# scaling\n",
    "def scaling_all(df):\n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(df)\n",
    "    return pd.DataFrame(X, columns=df.columns, index=df.index)\n",
    "\n",
    "def scaling_selective(df):\n",
    "    cols = ['experience', 'kidslt6', 'kids618', 'husby', 'education']  # Specify the columns to scale here\n",
    "    scaler = StandardScaler()\n",
    "    df_scaled = df.copy()\n",
    "    df_scaled[cols] = scaler.fit_transform(df[cols])\n",
    "    return pd.DataFrame(df_scaled, columns=df.columns, index=df.index)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def remove_mahalanobis_outliers(df, percentile=98):\n",
    "    \"\"\"\n",
    "    Remove outliers based on Mahalanobis distance from a DataFrame's numerical columns.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame.\n",
    "        percentile (float): The percentile to use as a threshold for identifying outliers (default is 98).\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame with outliers removed.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Step 1: Select only numerical columns\n",
    "    df_numeric = df.select_dtypes(include=[np.number])\n",
    "    \n",
    "    # Step 2: Calculate the mean vector and covariance matrix\n",
    "    mean_vector = df_numeric.mean(axis=0)\n",
    "    cov_matrix = np.cov(df_numeric.values.T)\n",
    "\n",
    "    # Step 3: Add a small regularization term to the covariance matrix\n",
    "    regularization_term = 1e-5 * np.eye(cov_matrix.shape[0])\n",
    "    cov_matrix += regularization_term\n",
    "    \n",
    "    # Step 4: Mahalanobis distance function\n",
    "    def mahalanobis_distance(row, mean_vector, cov_matrix):\n",
    "        diff = row - mean_vector\n",
    "        inv_cov_matrix = np.linalg.inv(cov_matrix)\n",
    "        md = np.sqrt(diff.T @ inv_cov_matrix @ diff)\n",
    "        return md\n",
    "    \n",
    "    # Step 5: Apply the Mahalanobis distance function to each row\n",
    "    df_filtered = df.copy()\n",
    "    df_filtered['mahalanobis'] = df_numeric.apply(lambda row: mahalanobis_distance(row, mean_vector, cov_matrix), axis=1)\n",
    "    \n",
    "    # Step 6: Determine the threshold for identifying outliers\n",
    "    threshold = np.percentile(df_filtered['mahalanobis'], percentile)\n",
    "    \n",
    "    # Step 7: Plot the distribution of Mahalanobis distances\n",
    "    plt.hist(df_filtered['mahalanobis'], bins=30, edgecolor='k', alpha=0.7, density=True, label='Mahalanobis Distance')\n",
    "    plt.axvline(threshold, color='r', linestyle='dashed', linewidth=1, label=f'Threshold (at {percentile}th percentile)')\n",
    "    plt.title('Distribution of Mahalanobis Distances')\n",
    "    plt.xlabel('Mahalanobis Distance')\n",
    "    plt.ylabel('Density')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    # Step 8: Identify and filter out the outliers\n",
    "    outliers = df_filtered[df_filtered['mahalanobis'] > threshold]\n",
    "    \n",
    "    if not outliers.empty:\n",
    "        print(\"Outliers found\")\n",
    "        print(f\"Number of outliers: {len(outliers)}\")\n",
    "    else:\n",
    "        print(\"No outliers found\")\n",
    "    \n",
    "    # Step 9: Remove outliers and drop the Mahalanobis column\n",
    "    df_filtered = df_filtered[df_filtered['mahalanobis'] <= threshold].drop(columns=['mahalanobis'])\n",
    "    \n",
    "    return df_filtered"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df_1 = scaling_all(df)\n",
    "df_autograder_1 = scaling_all(df_autograder)\n",
    "\n",
    "df_1.head()"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "visualize_data(df_1)"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df_scaled_2 = scaling_selective(df)\n",
    "df_2 = remove_mahalanobis_outliers(df_scaled_2)\n",
    "df_autograder_2_scaled = scaling_selective(df_autograder)\n",
    "df_autograder_2 = remove_mahalanobis_outliers(df_autograder_2_scaled)\n",
    "\n",
    "df_2.head()"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "visualize_data(df_2)"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''''\n",
    "* Create the data pipeline functions (Edlyn)\n",
    "\n",
    "- Preprocess data (PIPE 1, PIPE 2)\n",
    "    . Apply one hot encoding for region, race and hispanic. Make a column for race_nan and hispanic_nan. \n",
    "      People that decide to not fill these information might have a characteristic profile. So we can let the\n",
    "      model decide if it is important or not.\n",
    "\n",
    "    . Make remaining yes/no 1 and -1 (helps KNN, since the features are logically opoosite of each other)\n",
    "\n",
    "    . Make True/False 1 and 0 (When it is not just about \"yes/no\" give 1 to True and 0 to False)\n",
    "\n",
    "    . Process education column, nan becomes average \n",
    "\n",
    "    . You can apply remove first if you want\n",
    "\n",
    "\n",
    "- Regular scaling (PIPE 1) \n",
    "\n",
    "- Selective scaling (PIPE 2)\n",
    "\n",
    "- Data filtering with mahalanobis (PIPE 2)\n",
    "\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"orange\">  **Pipeline creation** </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = pd.read_csv('health_insurance_train_processed.csv')# Use the pipiline 1 function function instead of this file\n",
    "\n",
    "display(df_1.iloc[9:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2 = pd.read_csv('health_insurance_train_processed.csv') # Use the pipiline 2 function instead of this file\n",
    "\n",
    "display(df_2.iloc[9:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into seen and unseen while keeping it as a pandas dataframe\n",
    "fraction = 0.2  # 20% of the rows\n",
    "\n",
    "#-------------------PIPE 1-------------------\n",
    "df_unseen_1 = df_1.sample(frac = fraction, random_state=42) # Get 20% of random rows\n",
    "df_seen_1 = df_1.drop(df_unseen_1.index) # Get the remaining 80% of the rows\n",
    "\n",
    "X_seen_1 = df_seen_1.iloc[:, 1:]\n",
    "Y_seen_1 = df_seen_1.iloc[:, 0]\n",
    "\n",
    "X_unseen_1 = df_unseen_1.iloc[:, 1:]\n",
    "Y_unseen_1 = df_unseen_1.iloc[:, 0]\n",
    "\n",
    "#-------------------PIPE 2-------------------\n",
    "df_unseen_2 = df_2.sample(frac = fraction, random_state=42)\n",
    "df_seen_2 = df_2.drop(df_unseen_2.index)\n",
    "\n",
    "X_seen_2 = df_seen_2.iloc[:, 1:]\n",
    "Y_seen_2 = df_seen_2.iloc[:, 0]\n",
    "\n",
    "X_unseen_2 = df_unseen_2.iloc[:, 1:]\n",
    "Y_unseen_2 = df_unseen_2.iloc[:, 0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"orange\">  **Training Functions** </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#-------------------Dummy-------------------\n",
    "from sklearn.dummy import DummyRegressor\n",
    "def train_dummy_predictor(X, Y):\n",
    "    model = DummyRegressor(strategy='mean')\n",
    "    model.fit(X, Y)\n",
    "    return model\n",
    "\n",
    "#-------------------KNN-------------------\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "def train_knn_regressor(X, Y, param_grid):\n",
    "    model = KNeighborsRegressor(**param_grid)\n",
    "    model.fit(X, Y)\n",
    "    Y_pred = model.predict(X)\n",
    "    loss_values = [mean_absolute_error(Y, Y_pred)]\n",
    "    return model,loss_values\n",
    "\n",
    "#-------------------SGD-------------------\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "def train_sgd_regressor(X, Y, params):\n",
    "    model = SGDRegressor(**params)\n",
    "    epochs = params['max_iter']\n",
    "\n",
    "    loss_values = []\n",
    "    for epoch in range(epochs):\n",
    "        model.partial_fit(X, Y)\n",
    "        Y_pred = model.predict(X)\n",
    "        epoch_loss = mean_absolute_error(Y, Y_pred)\n",
    "        loss_values.append(epoch_loss)\n",
    "    \n",
    "    return model, loss_values\n",
    "\n",
    "#-----------Decision Tree-------------------\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "def train_decision_tree_regressor(X, Y, params):\n",
    "\n",
    "    '''params : dict\n",
    "        Dictionary of parameters to pass to DecisionTreeRegressor.'''\n",
    "    \n",
    "    # splitter = Supported strategies are \"best\" to choose the best split and \"random\" to choose the best random split.\n",
    "    # max_features = The number of features to consider when looking for the best split\n",
    "    # min_samples_split = The minimum number of samples required to split an internal node\n",
    "    # min_samples_leaf = The minimum number of samples required to be at a leaf node\n",
    "\n",
    "    model = DecisionTreeRegressor(**params,random_state = 42)\n",
    "    loss_values = []\n",
    "    \n",
    "    # Custom training loop with logging\n",
    "    for depth in range(1, params['max_depth'] + 1):\n",
    "        model.set_params(max_depth=depth)\n",
    "        model.fit(X, Y)\n",
    "        Y_pred = model.predict(X)\n",
    "        loss = mean_absolute_error(Y, Y_pred)\n",
    "        loss_values.append(loss)\n",
    "    \n",
    "    return model, loss_values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"orange\">  **Logistic Adjustment** </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def soft_max_adjustnent(value, target1 = -1.372319745743416, target2= 0.765866095571318 , gamma=5):\n",
    "    # Calculate the distance to each target\n",
    "    dist_to_target1 = abs(value - target1)\n",
    "    dist_to_target2 = abs(value - target2)\n",
    "    \n",
    "    # Transform the distance to probability\n",
    "    # Assumed the distance follows a Exponential distribution\n",
    "    prob_target1 = np.exp(-gamma * dist_to_target1)\n",
    "    prob_target2 = np.exp(-gamma * dist_to_target2)\n",
    "    \n",
    "    # Normalize the probabilities using soft max\n",
    "    total_prob = prob_target1 + prob_target2\n",
    "    prob_target1 /= total_prob\n",
    "    prob_target2 /= total_prob\n",
    "    \n",
    "    # Adjust the value based on the probabilities\n",
    "    adjusted_value = prob_target1 * target1 + prob_target2 * target2\n",
    "    \n",
    "    return adjusted_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"orange\">  **Create model dict and test dataframe** </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#----------Creation of models dict-----------\n",
    "models_dict_1 = {'KNN': None, 'SGD': None, 'Tree':None}\n",
    "models_dict_2 = {'KNN': None, 'SGD': None, 'Tree':None}\n",
    "\n",
    "for key in models_dict_1:\n",
    "    models_dict_1[key] = {'defalt' :None, 'best_param':None, 'best_model' :None, 'ensemble':None, 'best_ensemble':None}\n",
    "    models_dict_2[key] = {'defalt' :None, 'best_param':None, 'best_model' :None, 'ensemble':None, 'best_ensemble':None}\n",
    "\n",
    "#----------Creation of test dfs-----------\n",
    "test_df_1 = pd.DataFrame(index=['D','T','TA','ET','BET','BETA'],columns=['KNN','SGD','Tree'])\n",
    "test_df_2 = pd.DataFrame(index=['D','T','TA','ET','BET','BETA'],columns=['KNN','SGD','Tree'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"orange\">  **Store and test default models** </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################\n",
    "# ------ KNN REGRESSOR\n",
    "print(\"------ KNN Regressor -----\")\n",
    "model = KNeighborsRegressor()\n",
    "model.fit(X_seen_1, Y_seen_1)\n",
    "models_dict_1['KNN']['defalt'] = model\n",
    "test_df_1.loc['D','KNN'] = mean_absolute_error(Y_unseen_1, model.predict(X_unseen_1))\n",
    "\n",
    "model = KNeighborsRegressor()\n",
    "model.fit(X_seen_2, Y_seen_2)\n",
    "models_dict_2['KNN']['defalt'] = model\n",
    "test_df_2.loc['D','KNN'] = mean_absolute_error(Y_unseen_2, model.predict(X_unseen_2))\n",
    "\n",
    "\n",
    "####################################################################\n",
    "# ------ SGD REGRESSOR\n",
    "print(\"------ SGD Regressor -----\")\n",
    "model = SGDRegressor()\n",
    "model.fit(X_seen_1, Y_seen_1)\n",
    "models_dict_1['SGD']['defalt'] = model\n",
    "test_df_1.loc['D','SGD'] = mean_absolute_error(Y_unseen_1, model.predict(X_unseen_1))\n",
    "\n",
    "model = SGDRegressor()\n",
    "model.fit(X_seen_2,  Y_seen_2)\n",
    "models_dict_2['SGD']['defalt'] = model\n",
    "test_df_2.loc['D','SGD'] = mean_absolute_error(Y_unseen_2, model.predict(X_unseen_2))\n",
    "\n",
    "####################################################################\n",
    "# ------ DECISION TREE REGRESSOR\n",
    "print(\"------ Decision Tree Regressor -----\")\n",
    "model = DecisionTreeRegressor()\n",
    "model.fit(X_seen_1, Y_seen_1)\n",
    "models_dict_1['Tree']['defalt'] = model\n",
    "test_df_1.loc['D','Tree'] = mean_absolute_error(Y_unseen_1, model.predict(X_unseen_1))\n",
    "\n",
    "model = DecisionTreeRegressor()\n",
    "model.fit(X_seen_2, Y_seen_2)\n",
    "models_dict_2['Tree']['defalt'] = model\n",
    "test_df_2.loc['D','Tree'] = mean_absolute_error(Y_unseen_2, model.predict(X_unseen_2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"orange\">  **Perform grid search, test, store best models and parameters** </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def grid_search(X, Y, model, param_grid, cv=5):\n",
    "    \n",
    "    # cv = It determines the cross-validation splitting strategy used to evaluate the performance of the model for each combination of hyperparameters\n",
    "    # This means that the dataset will be split into 5 parts (folds). The model will be trained on 4 parts and tested on the remaining part.\n",
    "    # This process will be repeated 5 times, each time with a different part as the test set.\n",
    "    grid_search = GridSearchCV(estimator=model, param_grid=param_grid, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1)\n",
    "    \n",
    "    # Fit the model\n",
    "    print(\"Working on grid search\")\n",
    "    grid_search.fit(X, Y)\n",
    "    \n",
    "    # Get the best model and parameters\n",
    "    best_model = grid_search.best_estimator_\n",
    "    best_params = grid_search.best_params_\n",
    "    \n",
    "    print(f\"Best parameters: {best_params}\\n\")\n",
    "    \n",
    "    return best_model, best_params\n",
    "\n",
    "\n",
    "def search_train_test_store(model, param_grid, X_seen, Y_seen, X_unseen, Y_unseen, models_dict, model_name, test_df):\n",
    "    # Perform grid search\n",
    "    best_model, best_params = grid_search(X_seen, Y_seen, model, param_grid, cv=5)\n",
    "    models_dict[model_name]['best_model'] = best_model\n",
    "    models_dict[model_name]['best_param'] = best_params\n",
    "    \n",
    "    # Predict and calculate errors\n",
    "    Y_pred = best_model.predict(X_unseen)\n",
    "    test_df.loc['T', model_name] = mean_absolute_error(Y_unseen, Y_pred)\n",
    "    Y_pred_adjusted = np.array([soft_max_adjustnent(value) for value in Y_pred])\n",
    "    test_df.loc['TA', model_name] = mean_absolute_error(Y_unseen, Y_pred_adjusted)\n",
    "\n",
    "# Define parameter grids\n",
    "param_grid_KNN = {\n",
    "    'n_neighbors': [3, 5, 7, 9, 11, 13, 15, 17, 19, 21],\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'p': [1, 2]  # 1 for Manhattan distance, 2 for Euclidean distance\n",
    "}\n",
    "\n",
    "param_grid_Tree = {\n",
    "    'criterion': ['squared_error', 'friedman_mse', 'absolute_error'],\n",
    "    'splitter': ['random'],\n",
    "    'max_depth': [10, 15, 20, 25],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4, 8],\n",
    "    'max_features': [None, 'sqrt', 'log2']\n",
    "}\n",
    "\n",
    "param_grid_SGD = {\n",
    "    'alpha': [0.0001, 0.001, 0.01, 0.1],\n",
    "    'penalty': ['l2', 'l1', 'elasticnet'],\n",
    "    'learning_rate': ['constant', 'optimal', 'invscaling', 'adaptive'],\n",
    "    'eta0': [0.00001, 0.0001],\n",
    "    'max_iter': [50, 100, 200]\n",
    "}\n",
    "\n",
    "# Train and evaluate models\n",
    "print(\"------ KNN Regressor -----\")\n",
    "search_train_test_store(KNeighborsRegressor(), param_grid_KNN, X_seen_1, Y_seen_1, X_unseen_1, Y_unseen_1, models_dict_1, 'KNN', test_df_1)\n",
    "search_train_test_store(KNeighborsRegressor(), param_grid_KNN, X_seen_2, Y_seen_2, X_unseen_2, Y_unseen_2, models_dict_2, 'KNN', test_df_2)\n",
    "\n",
    "print(\"------ Decision Tree Regressor -----\")\n",
    "search_train_test_store(DecisionTreeRegressor(random_state=42), param_grid_Tree, X_seen_1, Y_seen_1, X_unseen_1, Y_unseen_1, models_dict_1, 'Tree', test_df_1)\n",
    "search_train_test_store(DecisionTreeRegressor(random_state=42), param_grid_Tree, X_seen_2, Y_seen_2, X_unseen_2, Y_unseen_2, models_dict_2, 'Tree', test_df_2)\n",
    "\n",
    "print(\"------ SGD Regressor -----\")\n",
    "search_train_test_store(SGDRegressor(random_state=42), param_grid_SGD, X_seen_1, Y_seen_1, X_unseen_1, Y_unseen_1, models_dict_1, 'SGD', test_df_1)\n",
    "search_train_test_store(SGDRegressor(random_state=42), param_grid_SGD, X_seen_2, Y_seen_2, X_unseen_2, Y_unseen_2, models_dict_2, 'SGD', test_df_2)\n",
    "\n",
    "# Display results\n",
    "display(test_df_1)\n",
    "display(test_df_2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"orange\">  **Ensemble training and validation tests** </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_models(X_seen,Y_seen,params, model_train_function, n_models = 10, val_size = 0.2):\n",
    "\n",
    "    models_training_loss = []\n",
    "    models_val_loss = []\n",
    "    model_list = []\n",
    "\n",
    "\n",
    "    for n in range(n_models):\n",
    "        X_train, X_val, Y_train, Y_val = train_test_split(X_seen, Y_seen, test_size= val_size, random_state= 42*n)\n",
    "\n",
    "        model, loss_values = model_train_function(X_train, Y_train, params)\n",
    "        \n",
    "        model_list.append(model)\n",
    "        models_training_loss.append(loss_values)\n",
    "\n",
    "        Y_pred = model.predict(X_val)\n",
    "        val_loss = mean_absolute_error(Y_val, Y_pred)\n",
    "        models_val_loss.append(val_loss)\n",
    "    \n",
    "    return model_list, models_training_loss, models_val_loss\n",
    "\n",
    "n_models = 30\n",
    "\n",
    "# Create a dataframe to store the validation loss of each model with the mean at the end\n",
    "Ensemble_val_loss = pd.DataFrame(index=range(n_models + 1))\n",
    "Ensemble_val_loss.rename(index={n_models: 'mean'}, inplace=True)\n",
    "\n",
    "print(Ensemble_val_loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env1Conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
